{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f001da4-48f5-47a3-abea-8103de28be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6516090-b590-4de0-9d15-40fb9e7a87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Denoising Autoencoder\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67acd20c-b1a8-41e1-b14a-1397661fc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-Net architecture for the generator\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        reconstruction = self.decoder(encoding)\n",
    "        return reconstruction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c57f867-1eb7-4521-820a-eb60580f1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f6033a-b9b2-46c8-b980-eb06741c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Functions\n",
    "def denoise_audio(autoencoder, noisy_audio, device='cuda'):\n",
    "    noisy_audio = noisy_audio.to(device)\n",
    "    with torch.no_grad():\n",
    "        denoised_audio = autoencoder(noisy_audio)\n",
    "    return denoised_audio\n",
    "\n",
    "def inpaint_audio(generator, denoised_audio, device='cuda'):\n",
    "    denoised_audio = denoised_audio.to(device)\n",
    "    with torch.no_grad():\n",
    "        inpainted_audio = generator(denoised_audio)\n",
    "    return inpainted_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a5a37d4-1892-4934-afee-5dc45458b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and reconstructed audio saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Class with Denoising and Inpainting\n",
    "class DistortedAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, distorted_audio_folder, fixed_length=16000):\n",
    "        self.distorted_audio_files = sorted(os.listdir(distorted_audio_folder))\n",
    "        self.distorted_audio_folder = distorted_audio_folder\n",
    "        self.fixed_length = fixed_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.distorted_audio_files)\n",
    "\n",
    "    def _process_audio(self, audio):\n",
    "        # Convert to mono if the audio has more than one channel\n",
    "        if audio.size(0) > 1:\n",
    "            audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "        # Pad or truncate audio to fixed length\n",
    "        if audio.size(1) > self.fixed_length:\n",
    "            audio = audio[:, :self.fixed_length]\n",
    "        elif audio.size(1) < self.fixed_length:\n",
    "            padding = self.fixed_length - audio.size(1)\n",
    "            audio = nn.functional.pad(audio, (0, padding), 'constant', 0)\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        distorted_path = os.path.join(self.distorted_audio_folder, self.distorted_audio_files[idx])\n",
    "        distorted_audio, _ = torchaudio.load(distorted_path)\n",
    "        distorted_audio = self._process_audio(distorted_audio)\n",
    "        return distorted_audio, self.distorted_audio_files[idx]\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    distorted_audio_folder = r\"E:\\UCSC Quarters\\1. First Quarter - Fall 2024\\CSE290D-Neural Computation\\Historical Music Audio Restoration\\HarmonyGAN\\fma_small\\fma_small\\000\"\n",
    "    output_folder = \"reconstructed_folder\"\n",
    "    model_save_folder = \"Models\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(model_save_folder, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load or define models\n",
    "    autoencoder = DenoisingAutoencoder().to(device)\n",
    "    generator = UNetGenerator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    # Load pre-trained models if available\n",
    "    # autoencoder.load_state_dict(torch.load('autoencoder.pth'))\n",
    "    # generator.load_state_dict(torch.load('generator.pth'))\n",
    "\n",
    "    dataset = DistortedAudioDataset(distorted_audio_folder)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for distorted_audio, file_name in dataloader:\n",
    "        distorted_audio = distorted_audio.to(device)\n",
    "\n",
    "        # Step 1: Denoise\n",
    "        denoised_audio = denoise_audio(autoencoder, distorted_audio, device)\n",
    "\n",
    "        # Step 2: Inpaint\n",
    "        reconstructed_audio = inpaint_audio(generator, denoised_audio, device)\n",
    "\n",
    "        # Save Reconstructed Audio\n",
    "        reconstructed_audio = reconstructed_audio.squeeze(0).detach().cpu().numpy()\n",
    "        reconstructed_audio = np.clip(reconstructed_audio, -1.0, 1.0)\n",
    "        sf.write(os.path.join(output_folder, file_name[0]), reconstructed_audio.T, 16000)\n",
    "\n",
    "    # Save Models\n",
    "    torch.save(autoencoder.state_dict(), os.path.join(model_save_folder, \"autoencoder.pth\"))\n",
    "    torch.save(generator.state_dict(), os.path.join(model_save_folder, \"generator.pth\"))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(model_save_folder, \"discriminator.pth\"))\n",
    "    print(\"Models and reconstructed audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d244d32-972f-424c-bac0-422650d33b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f979e-632d-4899-b3ec-310c8f09928b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
